{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IB Essay Plan Generator with OpenAI's GPT-4\n",
    "\n",
    "\n",
    "This notebook showcases a Python script that generates a chat between a subject expert and a critic to create a detailed plan for an IB essay. It leverages OpenAI's GPT-4 model to simulate a conversation, aiming to produce a comprehensive and academically rigorous essay plan.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Kaku requires `Python>=3.10`.  Additionally, ensure you have installed the autogen library:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "First, we import necessary libraries and modules. The script uses **re** for regular expressions, **os** for environment variable access, and **autogen** for creating automated agents and managing group chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from autogen import UserProxyAgent, AssistantAgent, GroupChat, GroupChatManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Chat Function\n",
    "The **generate_chat** function simulates a conversation between a subject expert and a critic, focusing on developing an IB essay plan. The conversation aims to ensure the essay plan is comprehensive, engaging, and academically rigorous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat(topic):\n",
    "    \"\"\"\n",
    "    Generate a chat between a subject expert and a critic to create an IB essay plan.\n",
    "\n",
    "    :param topic: The topic of the essay.\n",
    "    :return: A dictionary containing the chat messages.\n",
    "    \"\"\"\n",
    "    subject_expert_system_prompt = \"\"\"\n",
    "    As a Subject Expert in IB essay writing, your task is to guide the creation of a\n",
    "    comprehensive and engaging essay.\n",
    "    The focus is on detailed planning for each section,\n",
    "    ensuring academic rigor and relevance to the essay title. The plan should include:\n",
    "\n",
    "    Title: [Your Essay Title Here]\n",
    "\n",
    "    1. Introduction/Rationale (Word Count: XX)\n",
    "    - Purpose: Explain the choice of this topic.Begin the rationale by relating\n",
    "        the topic to a personal experience. Make up a personal story about what\n",
    "        motivates you to choose this topic. Conclude the introduction with a clear\n",
    "        statement of the research objective. Explicitly mention what specific\n",
    "        aspect or question you aim to investigate in your study\n",
    "\n",
    "\n",
    "    2. Background Information (Word Count: XX)\n",
    "    - Content: Outline basic mathematical formulas or models that will be further developed in the essay. Don't mention that model here, only basic knowledge\n",
    "    - If a graph is needed to illustrate concepts, annotate with '- GRAPH: [Describe the graph content]'.\n",
    "    - Make sure the model is within the syllybus of IB, if not, use simper model, only explains the model\n",
    "\n",
    "    3. Exploration (Total Word Count: XX)\n",
    "    3.1. Definition (Word Count: XX): Define key terms, particularly for mathematical models.\n",
    "            - If a graph is needed, annotate with '- GRAPH: [Describe the graph content]'.\n",
    "    3.2. Model Building (Word Count: XX): Enhance and refine the base model.\n",
    "            Break it into different sections, for each section, annoate with (Word Count: XX)\n",
    "            If a graph is needed, annotate with '- GRAPH: [Describe the graph content]'. Put it at the end\n",
    "            Format example:\n",
    "            Section A - Classic SIR Model Adaptation (Word Count: 500)\n",
    "            - Explanation of the SIR model adaptation to zombie dynamics, with zombies as the 'Infected' and fates of the 'Removed'.\n",
    "            - GRAPH: Transition diagram for the adapted SIR model including zombie-specific transitions.\n",
    "\n",
    "            Clearly specify the final model used in the Modeling section. Provide a\n",
    "            detailed description of the model, including its structure, components,\n",
    "            and how it was developed.\n",
    "            When deducing the formulas/models, make sure you clearly write out each step, how you plug-in values, combine functions, use what theorem, etc\n",
    "\n",
    "            Annotate each section with (Word Count: XX)\n",
    "\n",
    "    3.3. Experiment (Word Count: XX): Detail the process of data collection (whether real or simulated) and its application to the model.\n",
    "            Break it into different sections, for each section, annoate with (Word Count: XX)\n",
    "            If a graph is needed, annotate with '- GRAPH: [Describe the graph content]'.  Put it at the end\n",
    "            Format example:\n",
    "            Section A - Classic SIR Model Adaptation (Word Count: 500)\n",
    "            - Explanation of the SIR model adaptation to zombie dynamics, with zombies as the 'Infected' and fates of the 'Removed'.\n",
    "            - GRAPH: Transition diagram for the adapted SIR model including zombie-specific transitions.\n",
    "            - Try your best to design a meaningful experiment that would best reflect how the model defined in part 3.2 works\n",
    "            - Detail the process of data collection (whether real or simulated) and its application to the model.\n",
    "            - Write out the final conclusion for this experiment AND very importantly the insights from this experiment\n",
    "            - Annotate each section with (Word Count: XX)\n",
    "\n",
    "    4. Conclusion (Total Word Count: XX)\n",
    "    4.1. Summary (Word Count: XX): Recapitulate the main findings, conclusions, and insights. Address the initial question in the Rationale.\n",
    "    4.2. Reflection (Word Count: XX): Evaluate the strengths and weaknesses of the model and the essay.\n",
    "    4.3. Extension (Word Count: XX): Propose potential enhancements and future explorations given more time.\n",
    "\n",
    "    Ensure the plan is logical, detailed, and academically sound, with each section and subsection clearly connected to the essay's title. Incorporate critical feedback and format the plan within 'plan' tags without adding extra comments or brackets.\n",
    "    Please ensure the plan within the tag consists only of the main content for each section and subsection, directly addressing the essay's title without any additional sentences or explanations.\n",
    "\n",
    "    Make sure the graph is plotable with Python\n",
    "\n",
    "\n",
    "    REPLAY THE PLAN ONLY NO MATTER WHAT\n",
    "\n",
    "    If GRPAH is not needed, just don't use it, don't use N/A\n",
    "\n",
    "    PLease have the plan tag [plan] [/plan]\n",
    "\n",
    "    \"\"\"\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    config_list = [{'model': 'gpt-4-1106-preview', 'api_key': openai_api_key}]\n",
    "    subject_expert = AssistantAgent(\"subject_expert\", system_message=subject_expert_system_prompt, llm_config={\"config_list\": config_list})\n",
    "\n",
    "    critic_system_prompt = \"\"\"\n",
    "    Your role involves a thorough examination of the IB essay plan developed by a\n",
    "    subject expert. Your feedback is essential in enhancing the planâ€™s academic\n",
    "    strength, ensuring relevance to the essay's title, and maintaining a logical\n",
    "    structure. Focus on the following key areas:\n",
    "\n",
    "    Assess Each Section and Subsection:\n",
    "\n",
    "    Confirm that each part of the plan is well-defined and contributes meaningfully\n",
    "    to the essay's overall objective.\n",
    "    Check that word counts are included for each section and subsection, indicating\n",
    "    the depth and detail required.\n",
    "    Evaluate Graph Requirements:\n",
    "\n",
    "    For the Background Information and other relevant sections, assess if a graph is\n",
    "    necessary to illustrate concepts. When required, specify the type of graph and\n",
    "    annotate using 'GRAPH: [Describe the graph content]'.\n",
    "    In the Exploration section, particularly the Experiment subsection, provide\n",
    "    specific advice on the representation of data. If the topic is suitable for\n",
    "    simulated data, and it is preferable, annotate this requirement with\n",
    "    'GRAPH: [Describe the graph content]'. This option should be chosen when feasible.\n",
    "    Feedback Delivery:\n",
    "\n",
    "    Offer your feedback in a constructive, clear, and detailed manner.\n",
    "    Focus on enhancing the clarity, academic integrity, and overall effectiveness of\n",
    "    the essay plan.\n",
    "    Refrain from adding any comments or annotations that are not directly related to\n",
    "    improving the structure, content, and academic rigor of the plan.\n",
    "    Your critical analysis and recommendations are vital in refining the essay plan\n",
    "    to meet high academic standards and to ensure a coherent and impactful essay.\n",
    "    Make sure the graph is plotable with Python.\n",
    "\n",
    "    \"\"\"\n",
    "    critic = AssistantAgent(\n",
    "        name=\"Critic\",\n",
    "        system_message=critic_system_prompt,\n",
    "        llm_config={\"config_list\": config_list}\n",
    "    )\n",
    "\n",
    "    subject_expert_admin_system_prompt = \"\"\"\n",
    "    \"A human admin. Initialize the converstion. Alternate between critic and Subject Expert\n",
    "    \"\"\"\n",
    "    termination_msg = lambda x: isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "    subject_expert_admin1 = UserProxyAgent(\"Admin\", code_execution_config=False, system_message=subject_expert_admin_system_prompt, human_input_mode=\"NEVER\",  is_termination_msg=termination_msg)\n",
    "\n",
    "    groupchat = GroupChat(agents=[critic, subject_expert, subject_expert_admin1], messages=[], max_round=2)\n",
    "    manager = GroupChatManager(groupchat=groupchat, llm_config={\"config_list\": config_list})\n",
    "    subject_expert_admin1.initiate_chat(manager, message=f\"Write an IB essay {topic} with 4000 words.\")\n",
    "    \n",
    "    # print(f\"[DEBUG] Chat messages: {subject_expert_admin1.chat_messages}, type: {type(subject_expert_admin1.chat_messages)}\")\n",
    "    return subject_expert_admin1.chat_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Parsing the Essay Plan\n",
    "After generating the chat, we extract the latest essay plan and parse it into a structured dictionary format. This allows for easier manipulation and analysis of the plan's components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latest_plan(chat_messages):\n",
    "    \"\"\"\n",
    "    Extract the latest essay plan from the chat messages.\n",
    "    :param chat_messages: A dictionary containing the chat messages.\n",
    "    \"\"\"\n",
    "    for chat_key in chat_messages:\n",
    "        for message in reversed(chat_messages[chat_key]):\n",
    "            if '[plan]' in message['content']:\n",
    "                start_index = message['content'].find('[plan]') + len('[plan]')\n",
    "                end_index = message['content'].find('[/plan]', start_index)\n",
    "                if end_index != -1:\n",
    "                    return message['content'][start_index:end_index].strip()\n",
    "    return None\n",
    "\n",
    "def parse_document_to_dict(text):\n",
    "    \"\"\"\n",
    "    Parse the document into a dictionary.\n",
    "    :param text: The text of the document.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    idx = 0\n",
    "    res = {}\n",
    "    while idx < len(lines):\n",
    "      if 'Title' in lines[idx]:\n",
    "        extracted_text = lines[idx].split(\"Title: \")[1]\n",
    "        res['title'] = extracted_text\n",
    "        idx += 1\n",
    "        break\n",
    "      else:\n",
    "        idx += 1\n",
    "    idx_list = [idx]\n",
    "    while idx_list[0] < len(lines):\n",
    "      try:\n",
    "        extract_section(res, idx_list, lines)\n",
    "      except Exception as e:\n",
    "        print(f\"Error while extracting section: {e}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Plan Dictionary\n",
    "To facilitate working with the essay plan, we convert it into a dictionary. This approach enables structured access to different parts of the plan, such as the title, sections, and subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plan_dict(topic):\n",
    "    \"\"\"\n",
    "    Generate an IB essay plan dictionary for a given topic.\n",
    "    :param topic: The topic of the essay.\n",
    "    \"\"\"\n",
    "    chat_messages = generate_chat(topic)\n",
    "    latest_plan = extract_latest_plan(chat_messages)\n",
    "    latest_plan_dict = parse_document_to_dict(latest_plan)\n",
    "    return latest_plan,latest_plan_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Subsections and Subsubsections\n",
    "In the process of parsing the generated essay plan, it's crucial to accurately extract information from subsections and subsubsections. These functions delve deeper into the document's structure, pulling out detailed elements based on their formatting and content.\n",
    "\n",
    "### Extracting Subsubsections\n",
    "The **extract_subsubsection** function navigates through the content, identifying and extracting information from subsubsections within a section. It's designed to handle specific patterns that denote a subsubsection, such as \"Section A - ...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subsubsection(current_res, idx, lines, parent_number, pp_number):\n",
    "    while idx[0] < len(lines) and lines[idx[0]].strip() == '':\n",
    "        idx[0] += 1\n",
    "\n",
    "    section_check_pattern = r\"Section\\s+[A-Z]+\"\n",
    "    remove_section_pattern = r\"Section\\s+[A-Z]+\\s+-\\s+\"\n",
    "    content = []\n",
    "\n",
    "    if re.search(section_check_pattern, lines[idx[0]]):\n",
    "        modified_string = re.sub(remove_section_pattern, \"\", lines[idx[0]])\n",
    "        content.append(modified_string)\n",
    "        idx[0] += 1\n",
    "\n",
    "        while idx[0] < len(lines) and lines[idx[0]].strip() != '' and not re.search(section_check_pattern, lines[idx[0]]) and '3.3.' not in lines[idx[0]] and '4.' not in lines[idx[0]]:\n",
    "            if 'GRAPH:' in lines[idx[0]]:\n",
    "                lines[idx[0]] = lines[idx[0]].replace('GRAPH:', 'GRAPH_DESCRIPTION')\n",
    "            content.append(lines[idx[0]])\n",
    "            idx[0] += 1\n",
    "\n",
    "        current_res[pp_number]['sub_section'][parent_number]['content'].append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Subsections\n",
    "Similarly, the **extract_subsection** function is tasked with identifying and extracting subsections from the document. It looks for markers that indicate the start of a subsection and processes its content accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subsection(current_res, idx, lines, parent_number):\n",
    "    while idx[0] < len(lines) and lines[idx[0]] == '':\n",
    "        idx[0] += 1\n",
    "\n",
    "    number_match = re.search(r\"\\b\\d+\\.\\d+\\b\", lines[idx[0]])\n",
    "    number = number_match.group(0) if number_match else None\n",
    "    lines[idx[0]] = lines[idx[0]].replace(\"Total Word Count\", \"Word Count\")\n",
    "\n",
    "    if ('):' not in lines[idx[0]]):\n",
    "        lines[idx[0]] += ':'\n",
    "\n",
    "    pattern = r\"(\\w+) \\(Word Count: (\\d+)\\):(.*)\"\n",
    "    match = re.search(pattern, lines[idx[0]])\n",
    "    if match:\n",
    "        title, word_count, content = match.groups()\n",
    "    else:\n",
    "        title, word_count, content = None, None, None\n",
    "\n",
    "    if number not in ['3.2', '3.3']:\n",
    "        current_res[parent_number]['sub_section'][number] = {\n",
    "            'title': title,\n",
    "            'word_count': word_count,\n",
    "            'content': [content]\n",
    "        }\n",
    "\n",
    "        idx[0] += 1\n",
    "        if idx[0] >= len(lines) or lines[idx[0]] == '':\n",
    "            return\n",
    "\n",
    "        while idx[0] < len(lines) and '-' in lines[idx[0]] and 'GRAPH' not in lines[idx[0]] and '3.2' not in lines[idx[0]] and '3.3' not in lines[idx[0]] and '4.1' not in lines[idx[0]] and '4.2' not in lines[idx[0]] and '4.3' not in lines[idx[0]]:\n",
    "            current_res[parent_number]['sub_section'][number]['content'].append(lines[idx[0]])\n",
    "            idx[0] += 1\n",
    "\n",
    "        if idx[0] >= len(lines) or lines[idx[0]] == '':\n",
    "            return\n",
    "\n",
    "        graph_idx = lines[idx[0]].find('GRAPH')\n",
    "        if graph_idx != -1:\n",
    "            current_res[parent_number]['sub_section'][number]['graph_description'] = lines[idx[0]][graph_idx + 7:]\n",
    "            idx[0] += 1\n",
    "    else:\n",
    "        current_res[parent_number]['sub_section'][number] = {\n",
    "            'title': title,\n",
    "            'word_count': word_count,\n",
    "            'content': []\n",
    "        }\n",
    "        idx[0] += 1\n",
    "        section_check_pattern = r\"Section\\s+[A-Z]+\"\n",
    "        remove_section_pattern = r\"Section\\s+[A-Z]+\\s+-\\s+\"\n",
    "        while idx[0] < len(lines) and '3.3' not in lines[idx[0]] and '4.' not in lines[idx[0]]:\n",
    "            extract_subsubsection(current_res, idx, lines, number, parent_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Sections\n",
    "At a higher level, the **extract_section** function orchestrates the extraction of sections from the document. It uses the previously described functions to handle subsections and subsubsections, ensuring a comprehensive parsing of the document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section(current_res, idx, lines):\n",
    "    while idx[0] < len(lines) and lines[idx[0]] == '':\n",
    "        idx[0] += 1\n",
    "\n",
    "    number_match = re.search(r\"^\\b(\\d+(\\.\\d+)*)\\b\", lines[idx[0]])\n",
    "    number = number_match.group(0) if number_match else None\n",
    "    current_res[number] = {}\n",
    "    has_sub_section = \"Total Word Count\" in lines[idx[0]]\n",
    "\n",
    "    if not has_sub_section:\n",
    "        word_count_match = re.search(r\"Word Count: (\\d+)\", lines[idx[0]])\n",
    "        word_count = int(word_count_match.group(1)) if word_count_match else None\n",
    "        content = re.sub(r\"^\\d+(\\.\\d+)*\\.\\s+|\\s+\\(Word Count: \\d+\\)$\", \"\", lines[idx[0]])\n",
    "        current_res[number] = {\n",
    "            'title': content,\n",
    "            'word_count': word_count,\n",
    "            'content': []\n",
    "        }\n",
    "    else:\n",
    "        content = re.sub(r\"^\\d+(\\.\\d+)*\\.\\s+|\\s+\\(Total Word Count: \\d+\\)$\", \"\", lines[idx[0]])\n",
    "        current_res[number] = {\n",
    "            'title': content,\n",
    "            'sub_section': {}\n",
    "        }\n",
    "\n",
    "    idx[0] += 1\n",
    "    if not has_sub_section:\n",
    "        while idx[0] < len(lines) and lines[idx[0]] != '':\n",
    "            if 'GRAPH' in lines[idx[0]]:\n",
    "                current_res[number]['graph_description'] = lines[idx[0]][12:]\n",
    "            else:\n",
    "                current_res[number]['content'].append(lines[idx[0]][5:])\n",
    "            idx[0] += 1\n",
    "    else:\n",
    "        first_non_whitespace = next((char for char in lines[idx[0]] if not char.isspace()), None)\n",
    "        while idx[0] < len(lines) and lines[idx[0]] and first_non_whitespace == number:\n",
    "            extract_subsection(current_res, idx, lines, number)\n",
    "            while (idx[0] < len(lines) and lines[idx[0]].strip() == ''):\n",
    "                idx[0] += 1\n",
    "            if idx[0] >= len(lines):\n",
    "                break\n",
    "            first_non_whitespace = next((char for char in lines[idx[0]] if not char.isspace()), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Document to Dictionary\n",
    "\n",
    "After extracting the latest plan from the conversation, the next step involves parsing this plan into a structured dictionary. This process is facilitated by the **parse_document_to_dict** function, which reads through the plan text, identifies different sections, subsections, and their content, and organizes them into a nested dictionary. This structured approach allows for easy access to any part of the essay plan and facilitates further analysis or modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document_to_dict(text):\n",
    "    lines = text.split('\\n')\n",
    "    idx = 0\n",
    "    res = {}\n",
    "    while idx < len(lines):\n",
    "        if 'Title' in lines[idx]:\n",
    "            extracted_text = lines[idx].split(\"Title: \")[1]\n",
    "            res['title'] = extracted_text\n",
    "            idx += 1\n",
    "            break\n",
    "        else:\n",
    "            idx += 1\n",
    "    idx_list = [idx]\n",
    "    while idx_list[0] < len(lines):\n",
    "        try:\n",
    "            extract_section(res, idx_list, lines)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error while extracting section: {}\".format(e))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Outline Generation\n",
    "Finally, we provide a function to test the entire process of generating an essay plan and converting it to a dictionary. This function can be used to demonstrate the script's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_outline(topic):\n",
    "    chat_messages = generate_chat(topic)\n",
    "    latest_plan = extract_latest_plan(chat_messages)\n",
    "    latest_plan_dict = parse_document_to_dict(latest_plan)\n",
    "    return latest_plan,latest_plan_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook demonstrates how to automate the generation of an IB essay plan using OpenAI's GPT-4 model. By simulating a conversation between a subject expert and a critic, we aim to create a detailed and academically sound essay plan. The structured approach to parsing the plan into a dictionary facilitates further analysis and manipulation of the essay components.\n",
    "\n",
    "To utilize this in a Jupyter Notebook, copy the markdown and code segments into their respective cells. Ensure you have the necessary API key and library installed for the script to function properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
